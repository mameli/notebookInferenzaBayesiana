\subsubsection*{Esercizio 11.2 pag. 246 Hoff}

Randomized block design: researchers interested in identifying the optimal planting density for
a type of perennial grass performed the following randomized experiment: ten different plots of
land were each divided into eight subplots, and planting densities of 2, 4, 6 and 8 plants per
square meter were randomly assigned to the subplots, so that there are two subplots at each
density in each plot. At the end of the growing season the amount of plant matter yield was
recorded in metric tons per hectare. These data appear in the file pdensity.dat. The researchers
want to fit a model like $y = \beta_1 + \beta_2 x + \beta_3 x^2 + \epsilon$, where $y$ is yield and $x$ is planting density,
but worry that since soil conditions vary across plots they should allow for some across-plot
heterogeneity in this relationship. To accommodate this possibility we will analyze these data
using the hierarchical linear model described in Section 11.1.

\begin{enumerate}
    \item Before we do a Bayesian analysis we will get some ad hoc estimates of these parameters via
    least squares regression. Fit the model $y = \beta_1 +\beta_2 x+\beta_3 x^2 + \epsilon$ using OLS for each group,
    and make a plot showing the heterogeneity of the least squares regression lines. 
    From the least squares coefficients find ad hoc estimates of $\theta$  and $\Sigma$. 
    Also obtain an estimate of $\sigma$ 2 by combining the information from the residuals across the groups.
    \item Now we will perform an analysis of the data using the following distributions as prior distributions:
    $$ \Sigma^{-1} \sim \text{Wishart}(4, \hat\Sigma^{-1})$$
    $$ \theta \sim \text{multivariate normal} (\hat\theta, \hat\Sigma)$$
    $$ \sigma^2 \sim \text{inverse-gamma}(1, \hat\sigma^2)$$

    where $\hat\theta,|\hat\Sigma, \sigma^2 $ are the estimates you obtained in a). 
    Note that this analysis is not combining prior information with information from the data, as the “prior” distribution is based on
    the observed data. 
    However, such an analysis can be roughly interpreted as the Bayesian
    analysis of an individual who has weak but unbiased prior information.
    
    \item Use a Gibbs sampler to approximate posterior expectations of $\beta$ for each group $j$, and plot
    the resulting regression lines. Compare to the regression lines in a) above and describe why
    you see any differences between the two sets of regression lines.

    \item From your posterior samples, plot marginal posterior and prior densities of $\theta$ and the
    elements of $\Sigma$. 
    Discuss the evidence that the slopes or intercepts vary across groups.
    \item Suppose we want to identify the planting density that maximizes average yield over a
    random sample of plots. 
    Find the value $x_max$ of x that maximizes expected yield, and
    provide a 95\% posterior predictive interval for the yield of a randomly sampled plot having
    planting density $x_max$.
\end{enumerate}

\subsubsection*{Premessa e indicazioni generali}
Si vuole valutare la relazione tra raccolto e densità di piante
relativamente a 10 lotti di terra osservati nel seguente modo:

\begin{itemize}[-]
    \item 10 lotti di terra.
    \item Un lotto comprende altri 8 sottolotti.
    \item Densità di piantagione pari a 2, 4, 6 e 8 sono assegnate in maniera casuale tra
    gli 8 sottolotti in modo tale che ogni lotto ha due sottolotti di ognuna delle densità.
\end{itemize}

È richiesta l'analisi della relazione tra raccolto e densità mediante un modello di regressione
lineare quadratica e tenendo conto della variabilità tra gruppi in termini di condizioni di suolo.
Per come è costruito il disegno e per come è formulato il modello procediamo nel'analisi attraverso
un modello di regressione lineare gerarchico. Il setting del modello è:\\

\begin{itemize}[-]
	\item $ Y_{ij} = {\beta^T_jx_{ij}} + \varepsilon_{ij}$;
	\item $\varepsilon_{ij}|\sigma^2 \sim i.i.d. N(0,\sigma^2)$.
\end{itemize}

cioè:

\begin{itemize}[-]
	\item $Y_j | \beta_j, X_j, \sigma^2 \sim N_{n_j} ({X_j \beta_j}, \sigma^2 {I})$\quad
	$i = 1, \dots, n;\ \  j = 1, \dots, m$; 
	\item $ \beta_j | \theta, \Sigma \stackrel{iid}{\sim} N_{p} (\theta, \Sigma)$;
	\item $\theta| \mu_0,\Lambda_0 \sim N_p(\mu_0,\Lambda_0) $;
	\item $\Sigma \sim \text{\normalfont{Inverse - Wishart}}(\eta_0, S_0{-1})$;
	\item $ \sigma^2 | v_0, \sigma^2_0\sim \text{\normalfont{Inverse - Gamma}}(\frac{v_0}{2}, \frac{v_0\sigma^2_0}{2})$;
	\item $ Y_i \perp Y_j | \beta_1, \dots, \beta_m, \sigma^2 i \neq j$.
\end{itemize}

Il DAG relativo è mostrato in figura seguente.

\begin{tikzpicture}[
	> = stealth, % arrow head style
	shorten > = 1pt, % don't touch arrow head to node
	auto,
	node distance = 3cm, % distance between nodes
	semithick
]

\tikzstyle{every state}=[
	draw = black,
	thick,
	fill = white,
	minimum size = 4mm
]

\node[state, dashed] (v1)  {$\Theta$};
\node[state, dashed] (v2) [right of=v1] {$\Sigma$};

\node[state, dashed] (b2) [below of= v1] {$\beta_2$};
\node[state, dashed] (b1) [left of= b2] {$\beta_1$};
\node[state, dashed] (b3) [right of= b2] {$\beta_3$};
\node[state, dashed] (b4) [right of= b3] {$\beta_m$};

\node[state] (y1) [below of= b1] {$\underset{i=1 \dots n_1}{y_{i1}}$};
\node[state] (y2) [right of= y1] {$\underset{i=1 \dots n_2}{y_{i2}}$};
\node[state] (y3) [right of= y2] {$\underset{i=1 \dots n_3}{y_{i3}}$};
\node[state] (y4) [right of= y3] {$\underset{i=1 \dots n_m}{y_{im}}$};

\node[state] (x1) [below of= y1] {$x_1$};
\node[state] (x2) [right of= x1] {$x_2$};
\node[state] (x3) [right of= x2] {$x_3$};
\node[state] (x4) [right of= x3] {$x_m$};

\node[state, dashed] (s1) [below right=2cm and 0.8cm of x2] {$\sigma^2$};

\path[->] (v1) edge node {} (b1);
\path[->] (v1) edge node {} (b2);
\path[->] (v1) edge node {} (b3);
\path[->] (v1) edge node {} (b4);

\path[->] (v2) edge node {} (b1);
\path[->] (v2) edge node {} (b2);
\path[->] (v2) edge node {} (b3);
\path[->] (v2) edge node {} (b4);

\path[->] (b1) edge node {} (y1);
\path[->] (b2) edge node {} (y2);
\path[->] (b3) edge node {} (y3);
\path[->] (b4) edge node {} (y4);

\path[->] (x1) edge node {} (y1);
\path[->] (x2) edge node {} (y2);
\path[->] (x3) edge node {} (y3);
\path[->] (x4) edge node {} (y4);

\path[->] (s1) edge node {} (y1);
\path[->] (s1) edge node {} (y2);
\path[->] (s1) edge node {} (y3);
\path[->] (s1) edge node {} (y4);

\draw[black, dotted] (4, -3) -- (5, -3);
\draw[black, dotted] (4, -6) -- (5, -6);
\end{tikzpicture}

\newpage
In seguito si riporta il codice R.

\begin{lstlisting}[style=R]
rmvnorm<-function(n,mu, Sigma ) {
    p<-length(mu)
    res<-matrix ( 0, nrow=n, ncol=p)
    if ( n>0 & p>0 ) {
      E<-matrix( rnorm(n*p),n, p)
      res<-t ( t (E%*%chol ( Sigma ) ) +c (mu) )
      #R <- chol(A)
    }
    res
}
  
rinvwish <- function(n, nu0, iS0)
{
  sL0 <- chol(iS0)
  S <- array(dim = c (dim(iS0), n))
  for (i in 1:n)
  {
    Z <- matrix(rnorm(nu0 * dim(iS0)[1]), nu0, dim(iS0)[1]) %*% sL0
    S[, , i] <- solve(t(Z) %*% Z)
  }
  S[, , 1:n]
}
rwish <- function(n, nu0, S0)
{
  sS0 <- chol(S0)
  S <- array(dim = c(dim(S0), n))
  for (i in 1:n)
  {
    Z <- matrix(rnorm(nu0 * dim(S0)[1]), nu0, dim(S0)[1]) %*% sS0
    S[, , i] <- t(Z) %*% Z
  }
  S[, , 1:n]
}

#Lettura dei dati:

dati<-read.table("~/git/notebookInferenzaBayesiana/code/pdensity.dat", header=TRUE)
\end{lstlisting}
{
\color{red}
\begin{Verbatim}
> head (dati)

plot density yield
1       2     8.25
1       2     5.81
1       4     8.69
1       4     8.03
1       6     7.96
1       6     8.89
\end{Verbatim}
}

\begin{lstlisting}[style=R]
#Calcolo di quantita' utili per ogni gruppo ( numerosita', vettore delle osservazioni, matrice del modello e numero di parametri ):
ids<-unique (dati$plot )
m<-length ( ids )
Y<-list() ; X<-list() ; N<-NULL
for ( j in 1:m){
  Y[[ j ]]<-dati [ dati [,1]== ids [ j ], 3]
  N[j]<- sum( dati$plot==ids [ j ])
  xj<-dati [ dati [,1]== ids [ j ], 2]
  X[[ j ]]<-cbind ( rep (1,N[ j ]), xj, xj^2 )
}
p<-dim(X[[1]])[2]

\end{lstlisting}

{
\color{red}
\begin{verbatim}
> N

[1] 8 8 8 8 8 8 8 8 8 8
\end{verbatim}
}

{
\color{red}
\begin{verbatim}
> Y[ [ 1 ] ]

[1] 8.25 5.81 8.69 8.03 7.96 8.89 6.13 9.40    
\end{verbatim}
}

{
\color{red}
\begin{verbatim}
> X[ [ 1 ] ]

     xj
[1,] 1 2 4
[2,] 1 2 4
[3,] 1 4 16
[4,] 1 4 16
[5,] 1 6 36
[6,] 1 6 36
[7,] 1 8 64
[8,] 1 8 64
\end{verbatim}
}

\subsubsection*{Punto a)}
Stimiamo i coefficienti di regressione con il metodo dei minimi quadrati in modo indipendente per ciascuno dei 10 lotti.
\begin{lstlisting}[style=R]
S2.OLS<-BETA.OLS<-NULL
for( j in 1:m){
  fit<-lm(Y[[j]]~-1+X[[j]])
  BETA.OLS<-rbind (BETA.OLS, c(fit$coef ) )
  S2.OLS<-c ( S2.OLS, summary( fit )$sigma ^2)
}

colnames (BETA.OLS)<-c ("1"," x "," x^2")
\end{lstlisting}

{
\color{red}
\begin{Verbatim}
> BETA.OLS

            1       x         x^2
[1,]    4.84000 1.357250 -0.1243750
[2,]    4.53375 1.193375 -0.1290625
[3,]    2.07750 2.128250 -0.1643750
[4,]    2.60375 2.114875 -0.1928125
[5,]    3.57000 1.540500 -0.1500000
[6,]    1.47375 1.930875 -0.1215625
[7,]    3.96375 1.424875 -0.1278125
[8,]    0.52375 2.941875 -0.2653125
[9,]    3.36250 1.675500 -0.1400000
[10,]   1.73875 2.241125 -0.1771875
\end{Verbatim}
}

{
\color{red}
\begin{Verbatim}
> S2.OLS

[1] 1.8005320 1.0760545 0.8134580 0.5019505 0.5886680 0.8074545 0.9575905
[8] 0.3965025 0.1328380 0.8030505
\end{Verbatim}
}
Rappresentiamo graficamente le 10 linee di regressione per valutare la variabi\-lità tra gruppi. 
Riportiamo sul grafico anche la loro media.
\newpage
\begin{lstlisting}[style=R]
par (mfrow=c (1,2) )
plot ( range ( dati [,2]), range ( dati [,3]), type="n", xlab="planting density ", ylab="Expected yield ",main="Regression lines OLS")
for (j in 1:m) {
  curve (BETA.OLS[j, 1] + BETA.OLS[j, 2] * x + BETA.OLS[j, 3] * x ^ 2, col = "deepskyblue", lty = 2, add = T)
}
BETA.OLS.MEAN <- apply (BETA.OLS, 2, mean)
curve (BETA.OLS.MEAN[1] + BETA.OLS.MEAN[2] * x + BETA.OLS.MEAN[3] * x ^ 2,lwd = 2, col = "deeppink", add = T)
\end{lstlisting}

\begin{figure}
    \centering
    \includegraphics[totalheight=8cm]{img/esercizio11-2-1.png}
    \caption{ Curve di regressione con stime OLS.}
\end{figure}

{
\color{red}
\begin{Verbatim}
> BETA.OLS.MEAN

    1       x       x^2
2.86875 1.85485 -0.15925
\end{Verbatim}
}
\newpage
{
\color{red}
\begin{Verbatim}
> cov(BETA.OLS)

        1           x           x^2
1 2.00120764    -0.69321313  0.044309549
x -0.69321313    0.27555421 -0.020742679
x 2 0.04430955  -0.02074268  0.001968451
\end{Verbatim}
}

{
\color{red}
\begin{Verbatim}
> mean(S2.OLS)

[1] 0.7878099
\end{Verbatim}
}

Si osserva che:
\begin{itemize}[-]
    \item Tutte le curve hanno lo stesso andamento quadratico.
    \item Alcuni gruppi si discostano molto dalla media generale.
\end{itemize}

\subsubsection*{Punto b)}
Nel codice seguente impostiamo i parametri delle a priori semiconiugate per theta, Sigma e sigma2 come richiesto.
\begin{lstlisting}[style=R]
theta<-BETA.OLS.MEAN; Sigma<-cov(BETA.OLS) ; sigma2<-mean(S2.OLS)
eta0 <-4; S0<-Sigma
mu0<-theta ; L0<-Sigma
v0<-2; sigma20<-sigma2
\end{lstlisting}

\subsubsection*{Punto c)}
Impostiamo adesso i parametri delle a priori ed un Gibbs sampler, inizializ\-zando, in ordine, il numero di simulazioni, i valori iniziali dei parametri, e le strutture dati che serviranno per salvare i valori campionati dalle full conditional.
\begin{lstlisting}[style=R]
nsimul=10000
beta<-BETA.OLS
Sigma<-cov(BETA.OLS)
sigma2<-mean(S2.OLS)

Sigma.post<-matrix (0,p,p)
BETA.pp<-THETA.POST<-S2.POST<-NULL
BETA.POST<-BETA.OLS*0
SIGMA.POST<-array (0, c(p,p, nsimul ) )
set.seed (1)

for ( s in 1: nsimul ) {
  for ( j in 1:m){
    Vj<-solve ( solve (Sigma) + t(X[[j]])%*%X[[j]]/sigma2 )
    Ej<-Vj%*%( solve (Sigma)%*%theta + t(X[[j]])%*%Y[[j]]/sigma2 )
    beta [ j,]<-rmvnorm(1,Ej, Vj)
  }
  
  #theta 
  Lm<- solve ( solve (L0) + m* solve (Sigma) )
  mum<- Lm%*%( solve (L0)%*%mu0 + solve (Sigma)%*%apply ( beta,2,sum) )
  theta<-t (rmvnorm(1,mum,Lm) )
  
  #Sigma
  mtheta<-matrix ( theta,m,p, byrow=TRUE)
  Sigma<-solve ( rwish (1, eta0+m, solve ( S0+t ( beta-mtheta)%*%(beta-mtheta) ) ) )
  #Campioniamo la varianza residua:
  
  RSS<-0
  for ( j in 1:m) {
    RSS<-RSS+sum((Y[[j]]-X[[j]]%*%beta[j,] )^2 )
  }
  sigma2<-1/rgamma(1,( v0+sum(N) ) /2, (v0*sigma20+RSS)/2 )
  #Immagazziziniamo i valori appena campionati:
  S2.POST<-c(S2.POST, sigma2 ) ;THETA.POST<-rbind (THETA.POST, t ( theta ) )
  Sigma.post<-Sigma.post+Sigma ; BETA.POST<-BETA.POST+beta
  SIGMA.POST[,, s]<-Sigma
  #Campioniamo dalla posterior pdeeppinkictive dei coefficienti di regressione che ci servira' per il punto e).
  BETA.pp<-rbind (BETA.pp,rmvnorm(1, theta, Sigma) )
}

colnames (THETA.POST)<-c(" theta1 "," theta2 "," theta3 ")
colnames (BETA.POST)<-colnames (BETA.pp)<-c(" beta1 "," beta2 "," beta3 ")
BETA.PM<-BETA.POST/nsimul
plot (range (c (0, 10)), range (c (0, 10)), type = "n", xlab = "planting density ", ylab = "Yield ", main = "Bayesian regression lines ")
for (j in 1:m) {
  curve (BETA.PM[j, 1] + BETA.PM[j, 2] * x + BETA.PM[j, 3] * x ^ 2, lty = 2, col = "deepskyblue ", add = T)
}
curve (mean(THETA.POST[, 1]) + mean(THETA.POST[, 2]) * x + mean(THETA.POST[, 3]) * x ^ 2, lwd = 2, add = T, col = "deeppink")
\end{lstlisting}
Osservando i due grafici a confronto si nota che il modello gerarchico permette 
di trarre informazioni dai gruppi, 
riportando le curve di regressione lungo la media. 
Dal momento che lavoriamo con gruppi tutti di piccola numerosità c'è una grande 
variabilità nelle stime OLS mentre nel caso del modello gerarchico i gruppi si 
influenzano in termini di informazioni e per l'effetto di shrinkage la stima OLS 
viene portata verso la stima media in modo uguale per tutti i gruppi perché hanno 
la stessa numerosità campionaria.
\begin{figure}
    \centering
    \includegraphics[totalheight=8cm]{img/esercizio11-2-2.png}
    \caption{ GLMM: stime dei minimi quadrati e stime bayesiane a confronto.}
\end{figure}
\newpage
Controlliamo adesso la convergenza dell'algoritmo.
{
\color{red}
\begin{Verbatim}
> effectiveSize (THETA.POST)

theta1   theta2   theta3  
6938.395 7144.829 7501.045 
\end{Verbatim}
}

\subsubsection*{Punto d)}
Approssimazione delle distribuzioni a priori tramite simulazione Monte Carlo
\begin{lstlisting}[style=R]
n<-10000

THETA.PRIOR<-rmvnorm(n,mu0,L0) ; colnames (THETA.PRIOR)<-colnames (THETA.POST)
SIGMA.PRIOR<-rinvwish(n, eta0, solve( S0 ) )
\end{lstlisting}

\newpage
{
\color{red}
\begin{Verbatim}
> head(THETA.PRIOR)

           theta1      theta2     theta3
[1,]  -1.77780314    3.529232 -0.2613000
[2,]   3.47065885    1.752018 -0.1395134
[3,]   3.60717709    1.583506 -0.1350985
[4,]   0.07323521    2.928965 -0.2604753
[5,]   3.05388848    1.832459 -0.1571405
[6,]   2.27382048    1.786574 -0.1183835
\end{Verbatim}
}

{
\color{red}
\begin{Verbatim}
> head(SIGMA.PRIOR)

[1] 3.63026418 -0.93710237 0.04650011 -0.93710237 0.28263581 -0.01833983
\end{Verbatim}
}

\begin{lstlisting}[style=R]
#A priori e a posteriori di theta a confronto ( plot ):
par (mfrow = c(2, 2))
for (i in 1:3) {
  plot (density (THETA.POST[, i]), xlab = paste (" theta ", i), main = "", col = "deeppink")
  lines (density (THETA.PRIOR[, i]), col = "deepskyblue ")
  legend ("topright", legend = c(" Prior ", " Posterior "), col = c(" deepskyblue ", " deeppink "), lty = 1, bty = "n", cex = 0.7)
}
\end{lstlisting}

\begin{figure}
    \centering
    \includegraphics[totalheight=8.5cm]{img/esercizio11-2-3.png}
    \caption{  Densità a priori e a posteriori a confronto (media).}
\end{figure}

Si osserva che le distribuzioni a posteriori, pur essendo centrate sulla stessa 
media delle a priori (come ci aspettavamo dal momento che abbiamo centrato le 
prior nelle stime di massima verosimiglianza ), sono meno diffuse: 
in questo modo l'informazione a posteriori cambia nel senso che si dà maggiore 
probabilità ai valori che cadono attorno ad essa.

\begin{lstlisting}[style=R]
#Prior e posterior di Sigma a confronto ( plot ):

par (mfrow = c(2, 2))
for (i in 1:3) {
  plot (density (log(SIGMA.POST[i, i,])), type = "l", xlab = paste ("Sigma", i), col = "deeppink", ylab = "density ", main = "")
  lines (density (log(SIGMA.PRIOR[i, i,])), col = "deepskyblue ", lwd = 2)
  legend("topright", legend = c(" Prior ", " Posterior "), col = c(" deepskyblue ", " deeppink "), bty = "n", lty = 1)
}
\end{lstlisting}

Abbiamo preso il logaritmo degli elementi di Sigma dal momento che sono valori 
molto bassi. Dal momento che è richiesto di commentare la variabilitàa 
relativa all' intercetta e alle pendenze nei gruppi, 
riportiamo i plot delle distribuzioni dei soli elementi della diagonale principale di 
Sigma. 

La variabilità tra i gruppi delle intercette non è 
molto elevata, ma diminuisce ancora per il primo e per il secondo coefficiente.

\begin{figure}
  \centering
  \includegraphics[totalheight=8.5cm]{img/esercizio11-2-4.png}
  \caption{  Densità a priori e a posteriori a confronto (variabilità e covariabilità).}
\end{figure}

\subsubsection*{Punto e)}
Si vuole trovare la densità di piante che massimizza il raccolto atteso 
per un campione di lotti. Ricordiamo che stiamo lavorando con un modello 
gerarchico e quindi vogliamo tenere in considerazione anche la variabilità tra gruppi: 
ci serviamo a questo scopo della distribuzione predittiva a posteriori dei 
coefficienti di regressione (una normale multivariata con vettore delle medie e 
matrice di varianza e covarianza pari quelli estratti ad ogni passo dell'algoritmo ) 
per cui ogni vettore di coefficienti estratto rappresenta il vettore dei coefficienti 
di regressione per un gruppo futuro. Disponiamo già di tale distribuzione dal 
momento che è stata calcolata durante l'algoritmo. 
Necessario adesso confrontare 4 distribuzioni: quelle dei valori attesi del 
raccolto di un generico lotto, una per ogni valore osservato della covariata x. 
Anche in questo caso vogliamo tenere in considerazione la variabilità tra lotti e per 
questo approssimiamo le distribuzioni usando la predittiva a posteriori dei 
coefficienti di regressione appena discussa. Cerchiamo il valore atteso di y date 
le y passate e le x. Un modo per farlo è campionare i beta dalla loro posterior 
predictive e campionare le y dati i valori della x e quindi per ogni valore della x 
potevamo avere un valore atteso. Facciamo la media delle distribuzioni dei valori 
attesi. Campioniamo i beta tilde: i beta per un gruppo futuro. Per ogni valore di 
questo beta campinato abbiamo un valore del valore atteso per un y futuro. 
Alla fine per ogni x si genera una distribuzione del valore atteso della y futuro 
(4 vettori ). Confrontiamo le 4 distribuzioni ( per es con la media o vedendo la 
probabilità che una sia maggiore dell'altra ). Prendendo il valore medio per ogni valore 
di beta tilde e per x e la varianza a posteriori campiono un valore y. 
Così si incorpora anche l'incertezza derivante dai gruppi.
\newpage
\begin{lstlisting}[style=R]
x0<-c (2,4,6,8)
raccolto<-NULL
for ( i in x0){
  x<-c (1, i, i ^2)
  raccolto<-cbind ( raccolto,BETA.pp%*%x)
}
colnames ( raccolto )<-x0
\end{lstlisting}

{
\color{red}
\begin{Verbatim}
> head( raccolto )

            2        4        6        8
[1,] 5.989406 7.758386 8.317531 7.666840
[2,] 6.033526 7.921516 8.605911 8.086712
[3,] 6.350655 7.597301 7.712147 6.695192
[4,] 6.888414 7.776763 7.907158 7.279599
[5,] 5.973396 7.669427 8.482208 8.411740
[6,] 6.247427 7.685203 7.955526 7.058398
\end{Verbatim}
}
Confrontiamo adesso le quattro distribuzioni dei valori attesi,
graficamente e con un indice sintetico, la media:
\begin{lstlisting}[style=R]
par (mfrow = c (1, 1))
hist (raccolto [, 1], prob = T, main = "Raccolto atteso a posteriori ", xlab = "Raccolto ", xlim = c (2, 13), ylim = c (0, 1.5), col = "deepskyblue")
lines (density (raccolto [, 1]), col = "deepskyblue")
hist (raccolto [, 2], prob = T, col = "deeppink ", add = T)
lines (density (raccolto [, 2]), col = "deeppink")
hist (raccolto [, 3], prob = T, col = " chartreuse ", add = T)
lines (density (raccolto [, 3]), col = " chartreuse ")
hist (raccolto [, 4], prob = T, col = " darkorange ", add = T)
lines (density (raccolto [, 4]), col = "darkorange ")
legend ("topright",legend = c("x=2", "x=4", "x=6", "x=8"),col = c("deepskyblue", " deeppink ", "chartreuse", " lightsalmon "), lty = 1)
\end{lstlisting}

{
\color{red}
\begin{Verbatim}
> apply ( raccolto,2,mean)
       2        4        6        8
5.942984 7.739188 8.264142 7.517844
\end{Verbatim}
}

\begin{figure}
    \centering
    \includegraphics[totalheight=8cm]{img/esercizio11-2-5.png}
    \caption{  Valori attesi a posteriori per più valori di x a confronto.}
\end{figure}

Si osserva che il valore della covariata che massimizza il raccolto atteso per un 
generico lotto è x=6. Procediamo quindi considerando tale valore per il predittore 
lineare. Si vuole infine approssimare la distribuzione predittiva a posteriori per 
un generico lotto avendo una densità di piantagioni pari a 6. La logica è la stessa 
di quella usata per la predittiva a posteriori dei coefficienti di regressione, con 
la differenza che adesso siamo al livello più basso della gerarchia e quindi si 
aggiunge un parte di variabilità dovuta alle osservazioni campionarie. 
Per ogni vettore dei coefficienti estratto dalla predittiva a posteriori e per ogni 
elemento estratto dalla a posteriori della varianza residua ( anche questo fatto già 
fatto nell' algoritmo ) campioniamo un valore del raccolto da una normale con 
media pari al predittore lineare e varianza pari alla varianza residua. 
Riportiamo infine un plot della densità e l' intervallo di confidenza richiesto per 
tale distribuzione:

\begin{lstlisting}[style=R]
y.pred<-rnorm( nsimul,BETA.pp%*%x, S2.POST)
\end{lstlisting}

{
\color{red}
\begin{Verbatim}
> quantile (y.pred, c (0.025,0.975) 

    2.5%    97.5%
5.213170 9.855576
\end{Verbatim}
}

\begin{lstlisting}[style=R]
hist(y.pred, prob=T, main="Predittiva a posteriori ", xlab="raccolto ")
lines (y.pred )
\end{lstlisting}

\begin{figure}
    \centering
    \includegraphics[totalheight=10cm]{img/esercizio11-2-6.png}
    \caption{   Predittiva a posteriori.}
\end{figure}