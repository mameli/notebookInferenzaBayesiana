\subsection{Soluzione esercizio 8.2 Hoff}

Sensitivity analysis: In this exercise we will revisit the study from Exercise 5.2, in which 32 students in a science classroom were randomly assigned to one og two study methods, A and B, with $n_A = n_B = 16$. After several weeks of study, students were examined on the course material, and the scores are summarized by $\{\bar{y}_A = 75.2, s_A = 7.3\}, \{\bar{y}_B = 77.5, s_B = 8.1\}$. We will estimate $\theta_A = \mu+\delta$ and $\theta_B = \mu-\delta$ using the two-sample model and prior distributions of Section 8.1.
\begin{itemize}
	\item[a)] Let $\mu \sim $normal(75,100),  $1/\sigma^2\sim$ gamma(1,100) and $\delta\sim$normal$(\delta_0, \tau^2_0)$. For each combination of $\delta_0 \in \{-4, -2, 0, 2, 4\}$ and $\tau_0 \in \{10, 50, 100, 500\}$, obtain the posterior distribution of $\mu$, $\delta$ and $\sigma^2$ and compute
	\begin{itemize}
		\item[i.] Pr$(\delta>0|\boldsymbol{Y})$;
		\item[ii.] a 95$\%$ posterior confidence interval for $\delta$;
		\item[iii.] the prior and posterior correlation of $\theta_A$ and $\theta_B$.
	\end{itemize}
	\item[b)] Describe how you might use these results to convey evidence that $\theta_A<\theta_b$ to people of a variety of prior opinions.
\end{itemize}

\textbf{Svolgimento}
\bigskip

L'obiettivo di questo esercizio è quello di condurre un'analisi di sensitività. 
Ossia, si vuole valutare il cambiamento dell'inferenza al variare della specifica\-zione della prior. 
Dato il modello delle osservazioni:
\[
Y_{iA} = \mu + \delta + \varepsilon_{iA};
\]
\[
Y_{iB} = \mu - \delta + \varepsilon_{iB} = \varepsilon_{iB};
\]
\[
\varepsilon_{ij} | \sigma^2 \sim i.i.d \quad N(0,\sigma^2) \quad j = A,B;
\]
\[
\mu|\gamma_0^2,\mu_0 \sim N(\mu_0,\gamma_0^2);
\]
\[
\mu|\tau_0^2,\delta_0 \sim N(\delta_0,\tau_0^2);
\]
\[
\sigma^2 | \nu_0,\sigma_0^2 \sim inverse-Gamma(\frac{\nu_0}{2},\frac{\nu_0\sigma^2}{2})
\]
\[
p(\mu,\delta,\sigma^2) = p(u)p(\delta)p(\sigma^2)
\]
e la sua rappresentazione tramite DAG qui di seguito:

\begin{tikzpicture}[
	> = stealth, % arrow head style
	shorten > = 1pt, % don't touch arrow head to node
	auto,
	node distance = 3cm, % distance between nodes
	semithick
]

\tikzstyle{every state}=[
	draw = black,
	thick,
	fill = white,
	minimum size = 4mm
]

\node[state, dashed] (v1)  {$\mu$};

\node[state] (y1) [below left of= v1] {$y_{1A}$};
\node[state] (y2) [right of= y1] {$y_{1A}$};
\node[state] (y3) [right of= y2] {$y_{naA}$};
\node[state] (y4) [right of= y3] {$y_{1B}$};
\node[state] (y5) [right of= y4] {$y_{2B}$};
\node[state] (y6) [right of= y5] {$y_{nbB}$};

\node[state, dashed] (v2) [above right of=y4] {$\theta$};

\node[state, dashed] (s1) [below right=1cm and 0.8cm of y3] {$\sigma^2$};

\path[->] (v1) edge node {} (y1);
\path[->] (v1) edge node {} (y2);
\path[->] (v1) edge node {} (y3);
\path[->] (v1) edge node {} (y4);
\path[->] (v1) edge node {} (y5);
\path[->] (v1) edge node {} (y6);

\path[->] (v2) edge node {} (y1);
\path[->] (v2) edge node {} (y2);
\path[->] (v2) edge node {} (y3);
\path[->] (v2) edge node {} (y4);
\path[->] (v2) edge node {} (y5);
\path[->] (v2) edge node {} (y6);

\path[->] (s1) edge node {} (y1);
\path[->] (s1) edge node {} (y2);
\path[->] (s1) edge node {} (y3);
\path[->] (s1) edge node {} (y4);
\path[->] (s1) edge node {} (y5);
\path[->] (s1) edge node {} (y6);

\draw[black, dotted] (2.3, -2.2) -- (2.8, -2.2);
\draw[black, dotted] (10.9, -2.2) -- (11.4, -2.2);
\end{tikzpicture}

\begin{itemize}[-]
\item Assegniamo agli iperparametri i valori che sono stati richiesti
\[
\mu_0 = 75; \quad \gamma_0^2 = 100; \quad \frac{\nu}{2} = 1 \implies \nu_0 = 2; \quad \frac{\nu_0\sigma_0^2}{2}=100
\]
\[
\implies \sigma_0^2 = 100;
\]
\[
\delta_0 \in \{-4,-2,0,2,4\}; \quad \tau_0^2 \in \{10,50,100,500\}
\]
avendo i seguenti dati campionari
\[
\bar{y_A} = 75.2, \quad s_A = 7.3, n_A = 16;
\]
\[
\bar{y_B} = 77.5, \quad s_B = 8.1, n_B = 16;
\]
\[
n_j=n=16; \quad \bar{y_j}=\frac{1}{n_j}\sum_{i=1}^{n_j}y_{ij}; \quad s_j = \sqrt{\frac{1}{n_j-1}\sum_{i=1}^{n_j}(y_{ij}-\bar{y_j})^2}
\]
per fare l'inferenza richiesta è necessario determinare la distribuzione a posteriori delle variabili $\mu,\delta,\sigma^2$, ossia
$$p(\mu,\delta,\sigma^2|y_{1A},\dots,y_{n_AA},y_{1B},\dots,y_{n_BB}).$$ 
Per far ciò approssimiamo tale distribuzione mediante campionamento MCMC, secondo l'ottica Gibbs. Le distribuzioni full conditional dei parametri sono quindi le seguenti:
\[
\sigma^2 | y_{1A},\dots,y_{n_AA},y_{1B},\dots,y_{n_BB}, \mu, \delta \sim inverse-Gamma(\frac{\nu_n}{2},\frac{\nu_n\sigma_n^2}{2}) 
\]
in cui 
\[
\nu_n = \nu_0 + n_A + n_B;
\]
\[
\nu_n \sigma_n^2=\nu_0\sigma_0^2+\sum_{i=1}^{n_A}[y_{iA}-(\mu+\delta)]^2+ \sum_{k=1}^{n_B}[y_{kB}-(\mu-\delta)]^2=
\]
\[
=\nu_0\sigma_0^2 + \sum_{i=1}^{n_A}[y_{iA}^2-2(\mu+\delta)y_{iA} + (\mu + \delta)^2] +
\]
\[
 +\sum_{k=1}^{n_B}[y_{kB}^2-2(\mu-\delta) + (\mu - \delta)^2]=
\]
\[
=\nu_0\sigma_0^2 + \sum_{i=1}^{n_A}y_{iA}^2 - 2(\mu + \delta)\sum_{i=1}^{n_A}y_{iA} + n_A(\mu + \delta)^2+ 
\]
\[
+ \sum_{k=1}^{n_B}y_{kB}^2 - 2(\mu-\delta)\sum_{k=1}^{n_B}y_{kB} + n_B(\mu-\delta)^2=
\]
\[
\nu_0\sigma_0^2+(n_A-1)s_A^2+n_A\bar{y}_A^2-2(\mu + \delta)n_A\bar{y}_A+n_{A}(\mu+\delta)^2+
\]
\[
+(n_B-1)s_B^2+n_B\bar{y}_B^2-2(\mu-\delta)n_B\bar{y}_B+n_B(\mu-\delta)^2=
\]
\[
+n_B(\mu-\delta)^2=\nu_0\sigma_0^2 +(n-1)(s_A^2+s_B^2)+n(\bar{y}_A^2+\bar{y}_B^2)+
\]
\[
+n[-2(\mu+\delta)\bar{y}_A+(\mu+\delta)^2-2(\mu-\delta)\bar{y}_B+(\mu-\delta)^2=
\]
\[
\nu_0\sigma_0^2+(n-1)(s_A^2+s_B^2)+n(\bar{y}_A^2+\bar{y}_B^2)+
\]
\[
+2n[-\mu(\bar{y}_A+\bar{y}_B)+\delta(\bar{y}_B-\bar{y}_A)+\mu^2 + \delta^2].
\]
\[
\mu|y_{1A},\dots,y_{nA},y_{1B},\dots,y_{nB},\delta,\sigma^2 \sim N(\mu_n,\gamma_n^2)
\]
in cui 
\[
\gamma_n^2=[\frac{1}{\gamma_0^2}+\frac{(n_A+n_B)}{\sigma^2}]^{-1} = \frac{\gamma_0\sigma^2}{\sigma^2+(n_A+n_B)\sigma_0^2};
\]
\[
\mu_n =\gamma_n^2[\frac{\mu_0}{\gamma_0^2}+\frac{1}{\sigma^2}\sum_{i=1}^{n_A}(y_{iA}-\delta)+\frac{1}{\sigma^2}\sum_{k=1}^{n_B}(y_{kB}+\delta)]= 
\]
\[
=\gamma_n^2[\frac{\mu_0}{\gamma_0^2}+\frac{1}{\sigma^2}[(\sum_{i=1}^{n_A}y_{iA}) - n_A\delta + (\sum_{k=1}^{n_B}y_{kB} + n_B\delta)]]=
\]
\[
= \gamma_n^2[\frac{\mu_0}{\gamma_0^2}+\frac{1}{\gamma^2}(n_A\bar{y}_A-n_A\delta+n_B\bar{y}_B+n_B\delta)] = 
\]
\[
=\gamma_n^2[\frac{\mu_0}{\gamma_0^2}+\frac{n}{\sigma^2}(\bar{y}_A+\bar{y}_B)].
\]
\[
\delta | y_{1A},\dots,y_{n_aA},y_{1B},\dots,y_{n_BB},\mu,\delta^2 \sim N(\delta_n,\tau_n^2)
\]
in cui
\[
\tau_n^2 = (\frac{1}{\tau_0^2}+\frac{n_A+n_B}{\sigma^2})^{-1} = \frac{\sigma^2\tau_0^2}{\sigma^2+(n_A+n_B)\tau_0^2};
\]
\[
\delta_n = \tau_n^2[\frac{\delta_0}{\tau_0^2}+\frac{1}{\sigma^2}\sum_{i=1}^{n_A}(y_{iA}-\mu)-\frac{1}{\sigma^2}\sum_{k=1}^{n_B}(y_{kB}-\mu)]=
\]
\[
\tau_n^2[\frac{\delta_0}{\tau_0^2}+\frac{1}{\sigma^2}[(\sum_{i=1}^{n_A}y_{iA}-n_A\mu)+(\sum_{k=1}^{n_B}y_{kB}-n_B\mu)]]=
\]
\[
\tau_n^2[\frac{\delta_0}{\tau_0^2}+\frac{1}{\delta^2}(n_A\bar{y_A}-n_A\mu-n_B\bar{y_B})+n_B\mu)]=\tau_n^2[\frac{\delta_0}{\tau_0^2}+\frac{n}{\sigma^2}(\bar{y_A}-\bar{y_B})].
\]
Di seguito viene mostrato il codice R che svolge i punti a i), a ii) e a iii). Prima però viene calcolata la correlazione a priori tra $\theta_A$ e $\theta_B$ per il punto a iii):
\[
Corr(\theta_A,\theta_B) = \frac{Cov(\theta_a,\theta_B)}{\sqrt{Var(\theta_A)Var(\theta_B)}} = \frac{Cov(\mu+\delta,\mu-\delta)}{\sqrt{Var(\mu+\delta)Var(\mu-\delta)}}=
\]
\[
= \frac{Cov(\mu,\mu)+Cov(\mu,\delta)-Cov(\mu,\delta)-Cov(\delta,\delta)}{\sqrt{(Var(\mu)+Var(\delta))(Var(\mu)+Var(\delta))}}=
\]
\[
\frac{Var(\mu)-Var(\delta)}{\sqrt{(Var(\mu)+Var(\delta))^2}}
=\frac{\gamma_0^2-\tau_0^2}{\gamma_0^2+\tau_0^2}
\]
e quindi
\[
 \begin{matrix}
  \tau_0^2 & 10 & 50 & 100 & 500 \\
  Corr(\theta_A,\theta_B) & 0.81 & 0.33 & 0 & -0.67
 \end{matrix}
\]

\begin{lstlisting}[style=R]
#Quantita' campionarie a disposizione per i due
#gruppi (medie, deviazioni standard e numerosita'):
y.barA<-75.2
y.barB<-77.5
sA<-7.3
sB<-8.1
nA<-nB<-n_<-16

#Setting iperparametri delle prior: gli iperparametri 
#media e varianza(rispettivamente chiamati delta0 e tau20) 
#per la a priori di delta, la semidifferenza tra
#le medie dei due gruppi, sono due vettori dal momento
#che l'obbiettivo dell' esercizio e' quello di valutare se 
#e come cambia l'inferenza a seconda della prior specificata 
#proprio su delta.

delta0<-c(-4 ,-2 ,0 ,2 ,4)
tau20<-c(10, 50, 100, 500)
mu0<-75
gamma20<-100
v0<-2
sigma20<-100

#Valori iniziali dei parametri e numero di simulazioni:

delta <-(y.barA-y.barB)/2
mu<-(y.barA+y.barB) /2

nsimul<-1000

#Creiamo un array dei i valori campionati durante l'algoritmo:

gibbs<-array(NA,c(nsimul,3,length(delta0)*length(tau20)))

#Ciclo Gibbs:

v<-1
for(j in delta0){
	for(k in tau20){
		for(i in 1:nsimul) {

#Aggiorniamo sigma:

			vn<-v0+nA+nB
			vnsigma2n<-v0*sigma20+(n_-1)*(sA^2+sB^2)+n_*(y.barA^2+y.barB^2)+2*n_*(mu^2
			delta^2-mu*(y.barA+y.barB)+delta*(y.barB-y.barA))
			sigma2<-1/rgamma(1,vn /2,vnsigma2n/2)

#Aggiorniamo mu:

			gamma2n<-gamma20*sigma2/(sigma2+(nA+nB)*gamma20)
			mun<-gamma2n*(mu0/gamma20+n_*(y.barA+y.barB)/sigma2)
			mu<-rnorm(1,mun,sqrt(gamma2n))

#Aggiorniamo delta:

			tau2n<-k*sigma2 /(sigma2+(nA+nB)*k)
			deltan<-tau2n*(j/k+n_*(y.barA-y.barB)/sigma2)
			delta<-rnorm(1,deltan,sqrt(tau2n))

			gibbs[i,,v]<-c(mu,delta,sigma2)
		}
		v<-v+1
	}
}

colnames(gibbs)<-c("mu","delta","sigma2")

#a)

probabilita.post<-corrrelazione. post<-matrix (NA,5,4)
quantili.post<-NULL
v=1
for(i in 1:5){
  for(j in 1:4){
    probabilita.post[i,j]<-mean(gibbs[,2,v]<0)
    quantili.post<-rbind(quantili.post,quantile(gibbs[,2,v],c(0.025,0.975)))
    correlazione.post[i,j]<-cor(gibbs[,1,v]+gibbs[,2,v],
    gibbs[,1,v]-gibbs[,2,v])
    v<-v+1
  }
}

rownames(probabilita.post)<-rownames(correlazione.post)<-
c("delta0=-4",+"delta0=-2","delta0=0","delta0=2","delta0=4")
colnames(probabilita.post)<-colnames(correlazione.post)<-
c("tau20=10",+" tau20=50"," tau20=100"," tau20=500")
rownames(quantili.post)<-c("delta0=-4 tau20=10"," delta0=-4 tau20=50",
"delta0=-4 tau20=100","delta0=-4 tau20=500","delta0=-2 tau20=10",
"delta0=-2 tau20=50","delta0=-2 tau20=100","delta0=-2 tau20=500",
"delta0=0 tau20=10","delta0=0 tau20=50","delta 0=0 tau20=100",
"delta0=0 tau20=500","delta0=2 tau20=10","delta0=2 tau20=50",
"delta0=2 tau20=100","delta0=2 tau20=500","delta0=4 tau20=10",
"delta0=4 tau20=50","delta0=4 tau20=100","delta0=4 tau20=500")

#i)

probabilita.post
\end{lstlisting}

{
\color{red}
\begin{Verbatim}
> probabilita.post
          tau20=10    tau20=50  tau20=100  tau20=500
delta0=-4    0.899     0.834      0.807      0.802
delta0=-2    0.841     0.812      0.778      0.788
delta0=0     0.755     0.802      0.772      0.786
delta0=2     0.704     0.769      0.788      0.799
delta0=4     0.617     0.756      0.771      0.781
\end{Verbatim}
}
La tabella precedente contiene la probabilità a posteriori che la semidif\-ferenza tra le medie dei due gruppi sia negativa, 
per ciascuna delle possibile coppie di valori di $\delta_0$ e $\tau_o^2$.

\begin{lstlisting}[style=R]
#Rappresentiamo tutte queste probabilita' anche graficamente:

par(mfrow=c(3,2))
for(i in 1:5){
  plot(tau20,probabilita.post[i,],pch=20, xlab=expression(tau^2[0]),
  ylab=expression(p(delta<0 given bold(Y)), type="l")
  legend("right",paste("delta0=",delta0[i]), bty="n")
}

\end{lstlisting}

\begin{figure}[!ht]
 \centering
 \includegraphics[scale = 0.7]{img/esercizio8-2-1}
 \caption{probabilità a posteriori al variare della prior}
 \label{figure:figura12}
\end{figure}
\newpage
\begin{lstlisting}[style=R]
#a)ii
\end{lstlisting}
{
\color{red}
\begin{Verbatim}
> quantili.post
                        2.5%    97.5%
delta0=-4 tau20=10  -4.166501 0.885176
delta0=-4 tau20=50 -4.080470 1.261487
delta0=-4 tau20=100 -3.966220 1.815875
delta0=-4 tau20=500 -3.984476 1.560537
delta0=-2 tau20=10  -3.921579 1.224411
delta0=-2 tau20=50  -3.987036 1.520727
delta0=-2 tau20=100 -3.956075 1.720433
delta0=-2 tau20=500 -3.794180 1.702581
delta0=0 tau20=10   -3.459266 1.618012
delta0=0 tau20=50   -3.884302 1.613191
delta 0=0 tau20=100 -4.000856 1.521053
delta0=0 tau20=500  -4.081757 1.850854
delta0=2 tau20=10   -3.017789 1.890537
delta0=2 tau20=50   -3.727812 1.800686
delta0=2 tau20=100  -3.664780 1.952699
delta0=2 tau20=500  -4.089526 1.830220
delta0=4 tau20=10   -2.637665 2.135446
delta0=4 tau20=50   -3.850333 1.857685
delta0=4 tau20=100  -3.661231 1.695296
delta0=4 tau20=500  -4.298413 1.823357
\end{Verbatim}
}

\begin{lstlisting}[style=R]
#a)iii

#Confrontiamo ora le correlazioni a priori e quelle a
#posteriori fra le medie dei due gruppi thetaA=mu+delta e 
#thetaB=mu-delta.

correlazione.prior<-c(0.81,0.33,0,-0.67)

#Le correlazioni a posteriori precedentemente calcolate invece sono:
}

\end{lstlisting}

{
\color{red}
\begin{Verbatim}
> correlazione.post
              tau20=10    tau20=50    tau20=100    tau20=500
  delta0=-4 0.11470736  0.04394125 -0.014843283 -0.047380931
  delta0=-2 0.05693164  0.04900340 -0.081550575 -0.007036669
  delta0=0  0.06714184  0.03791250 -0.008966994 -0.026858228
  delta0=2  0.11899453  0.04389222 -0.009447653 -0.044961021
  delta0=4  0.13144920 -0.03688193  0.005192835 -0.064853163
\end{Verbatim}
}


\begin{lstlisting}[style=R]
#b)

win.graph()
par(mfrow=c(2,2))
x<-seq(-50,50,by=0.01)
v<-0
for(j in 1:4){
  plot(x,dnorm(x,delta0[1],sqrt(tau20[j])), xlim=c(-10,10),ylim=c(0,0.3),
  xlab=expression(delta),ylab="density",type="l",col="grey")
  lines(density(gibbs[,2,j]))
  legend("topright",legend=c("posterior","prior"),lwd=c(2,2),col=c("black",
  "gray"),bty="n")
  text(5.5,0.15,paste("tau20=",tau20[j]))
}

win.graph()
par(mfrow=c(2,2))
x<-seq(-50,50,by=0.01)
v<-0
for(j in 1:4){
  plot(x,dnorm(x,delta0[5],sqrt(tau20[j])),xlim=c(-10 ,10),ylim=c(0,0.3),
  xlab=expression(delta),ylab="density",type="l",col="grey")
  lines(density(gibbs[,2,j+16]))
  legend("topright",legend=c("posterior","prior"),lwd=c(2,2),col=c("black",
  "gray"),bty="n")
  text(5.5,0.15, paste("tau20=",tau20[j]))
}

\end{lstlisting}
\begin{figure}
 \centering
 \includegraphics[scale=0.7]{img/esercizio8-2-2}
 \caption{densita' a posteriori al variare della prior}
 \label{figure:figura13}
\end{figure}

\begin{figure}
 \centering
 \includegraphics[scale=0.7]{img/esercizio8-2-3}
 \caption{densita' a posteriori al variare della prior}
 \label{figure:figura14}
\end{figure}

\begin{lstlisting}[style=R]
#Facciamo una veloce verifica della convergenza dell'algoritmo:
#library(coda)
effectiveSize(gibbs[,,1])
\end{lstlisting}

{
\color{red}
\begin{Verbatim}
  mu  delta  sigma2
1000   1000    1000
\end{Verbatim}
}

\end{itemize}

